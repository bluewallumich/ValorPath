{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"name":"working-VA_NE_Region_FacilityData_On_ModelGardenGemai.ipynb"},"environment":{"kernel":"conda-root-py","name":"workbench-notebooks.m115","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/workbench-notebooks:m115"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bc8e6eeae29a47fe9de148a883628259":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_577d9ce377e84f508a52654cdd1d83e7"],"layout":"IPY_MODEL_01aab735a4454dc08b483d03e6b2c32a"}},"0345a3af10284c8b862be12599a2d2d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e38a90a692d4b1ca2f79506d02f39b9","placeholder":"​","style":"IPY_MODEL_392747cf17c543f693e6db47c7cc3ed1","value":"<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"}},"51626f8e15cd4417a05348cc91583090":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Username:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_35d09bdc3b444efda1ec7d877891da80","placeholder":"​","style":"IPY_MODEL_54615d55370e48059ecfd9bef04977c3","value":"bluewall"}},"bf044f766b7448e8bffa98bc36e5a9e6":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_2032be21bd674317b09eba49e4fe96f3","placeholder":"​","style":"IPY_MODEL_8c4c7316fea7452db80159026de02039","value":""}},"3b2fb9574c814b83b5e2962c0a7cd6d3":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_3b0f78f91f584873abd270166ad67a8a","style":"IPY_MODEL_94a5702d3fdd421d849d090ea10dab36","tooltip":""}},"849b11e345424b359afedf4d21fadd24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_550f6ecca1104db4a2bbc5a47349524f","placeholder":"​","style":"IPY_MODEL_b1e782263f4143cd98200361d7e304bf","value":"\n<b>Thank You</b></center>"}},"01aab735a4454dc08b483d03e6b2c32a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"5e38a90a692d4b1ca2f79506d02f39b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"392747cf17c543f693e6db47c7cc3ed1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35d09bdc3b444efda1ec7d877891da80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54615d55370e48059ecfd9bef04977c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2032be21bd674317b09eba49e4fe96f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c4c7316fea7452db80159026de02039":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b0f78f91f584873abd270166ad67a8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94a5702d3fdd421d849d090ea10dab36":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"550f6ecca1104db4a2bbc5a47349524f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1e782263f4143cd98200361d7e304bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19ac5b0d7d7c489eb555e9fdfef2fe4d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7e5262a7dc841a1917fd9169932ef01","placeholder":"​","style":"IPY_MODEL_148146db486346289658ca3948b10223","value":"Connecting..."}},"a7e5262a7dc841a1917fd9169932ef01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"148146db486346289658ca3948b10223":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"577d9ce377e84f508a52654cdd1d83e7":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fffbb3bc381417492641d875a0bc8fe","placeholder":"​","style":"IPY_MODEL_b5653928e7c64bc7a74a836f7404efce","value":"Kaggle credentials successfully validated."}},"2fffbb3bc381417492641d875a0bc8fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5653928e7c64bc7a74a836f7404efce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":169018,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":143795,"modelId":166379}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Copyright 2024 Google LLC\n\n#\n\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n\n# you may not use this file except in compliance with the License.\n\n# You may obtain a copy of the License at\n\n#\n\n#     https://www.apache.org/licenses/LICENSE-2.0\n\n#\n\n# Unless required by applicable law or agreed to in writing, software\n\n# distributed under the License is distributed on an \"AS IS\" BASIS,0000\n\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\n# See the License for the specific language governing permissions and\n\n# limitations under the License.","metadata":{"id":"ur8xi4C7S06n"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> This notebook was tested in the following environment:\n\n>\n\n> - Python 3.10\n\n> - Colab Enterprise with a `e2-standard-16` runtime:\n\n>   - 62.8 GB of system RAM\n\n>   - 0 GB of GPU RAM (NVIDIA L4)\n","metadata":{"id":"iYQE9Mza89tj"}},{"cell_type":"markdown","source":"## Overview\n","metadata":{"id":"eIVltG2O2qgF"}},{"cell_type":"markdown","source":"Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models.\n\n\n\nThis notebook demonstrates loading, finetuning, converting, and deploying Gemma to Vertex AI.\n","metadata":{"id":"tvgnzT1CKxrO"}},{"cell_type":"markdown","source":"### Objective\n\n\n\n- Load Gemma using KerasNLP\n\n- Finetune Gemma using KerasNLP\n\n- Convert Gemma to Hugging Face Transformers\n\n- Deploy Gemma to Vertex AI\n","metadata":{"id":"d975e698c9a4"}},{"cell_type":"markdown","source":"### Costs\n\n\n\nThis tutorial uses billable components of Google Cloud:\n\n\n\n- Vertex AI\n\n- Cloud Storage\n\n\n\nLearn about [Vertex AI](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage](https://cloud.google.com/storage/pricing) pricings,\n\nand use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n\nto generate a cost estimate based on your projected usage.\n","metadata":{"id":"aed92deeb4a0"}},{"cell_type":"markdown","source":"### Current Model State:\n\n- The model was trained using facility-specific health data, focusing on trends related to veterans' physical and mental health.\n\n- The last training session processed **102/10,205** examples, but to enhance the model’s accuracy, additional data and retraining will be required.\n\n\n\n### Disclaimer:\n\nPlease note that this notebook is a **work in progress** and may appear incomplete. Ongoing updates and improvements will be made as the model is retrained and further refined.\n\n\n\n### Healthcare Disclaimer:\n\nThe outputs of this model are intended for research and educational purposes only and should not be used as a substitute for professional medical advice, diagnosis, or treatment. Consult a qualified healthcare provider for any medical concerns or decisions related to patient care.\n\nrvices.\n\ny.","metadata":{"id":"1U_uMUlxRhth"}},{"cell_type":"markdown","source":"## Installation\n","metadata":{"id":"OrDeaJ_3vL0-"}},{"cell_type":"markdown","source":"Install the following packages required to execute this notebook:\n","metadata":{"id":"i7EUnXsZhAGF"}},{"cell_type":"code","source":"!pip install --upgrade keras-nlp tensorflow tensorflow-addons tensorflow-text kaggle kagglehub -q\n","metadata":{"id":"SpeKcQcw5E5T","executionInfo":{"status":"ok","timestamp":1731202742988,"user_tz":300,"elapsed":57969,"user":{"displayName":"","userId":""}},"outputId":"3bb808a8-5283-4b01-cab1-d8a575f50b4e"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Before you begin\n","metadata":{"id":"JaKPnjQsWiLn"}},{"cell_type":"markdown","source":"### Kaggle credentials\n\n\n\n> Add blockquote\n\n\n\n\n","metadata":{"id":"9xWLhlsd09zY"}},{"cell_type":"markdown","source":"Gemma models are hosted by Kaggle. To use Gemma, request access on Kaggle:\n\n\n\n- Sign in or register at [kaggle.com](https://www.kaggle.com)\n\n- Open the [Gemma model card](https://www.kaggle.com/models/google/gemma) and select _\"Request Access\"_\n\n- Complete the consent form and accept the terms and conditions\n\n\n\nThen, to use the Kaggle API, create an API token:\n\n\n\n- Open the [Kaggle settings](https://www.kaggle.com/settings)\n\n- Select _\"Create New Token\"_\n\n- A `kaggle.json` file is downloaded. It contains your Kaggle credentials\n\n\n\nRun the following cell and enter your Kaggle credentials.\n","metadata":{"id":"2y1Zf6OcvDxU"}},{"cell_type":"code","source":"import kagglehub\n\n\n\nkagglehub.login()","metadata":{"id":"TxmBScR4vjkf","executionInfo":{"status":"ok","timestamp":1731202744666,"user_tz":300,"elapsed":1684,"user":{"displayName":"","userId":""}},"outputId":"bd92b715-3492-4937-cc43-556c72c31804"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Note: If `kagglehub.login()` doesn't work for you, an alternative way is to set `KAGGLE_USERNAME` and `KAGGLE_KEY` environment variables.\n","metadata":{"id":"y8hNHhFkjMaf"}},{"cell_type":"markdown","source":"### Dependencies\n","metadata":{"id":"uQDEhMB_3kbE"}},{"cell_type":"code","source":"# Utility Libraries\n\nimport os\n\nimport datetime\n\nimport json\n\nimport csv\n\n\n\n# Data Handling\n\nimport pandas as pd\n\n\n\n# Machine Learning and Deep Learning\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\n\nfrom tensorflow.keras import mixed_precision\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom tensorflow.keras.optimizers import Adam\n\nimport keras_nlp  # if specifically using Keras NLP functionalities\n\nimport transformers  # if using models from Hugging Face's Transformers library\n\nimport torch\n\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n\n# Google Cloud Integration (if needed)\n\n    # from google.cloud import aiplatform\n\n    # from google.colab import files\n\n\n\n# Hugging Face Integration (if needed)\n\n    # from huggingface_hub import HfApi, HfFolder\n","metadata":{"id":"DVQhjWte3s7M","executionInfo":{"status":"ok","timestamp":1731202832412,"user_tz":300,"elapsed":11907,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset\n\n\n\nTo finetune Gemma, this notebook uses the [Veteran Affairs NE Regional](https://huggingface.co/datasets/vetHealthGuy/Veteran_Affairs_NorthEast_Region_Conversational_Dataset) test dataset.\n\n\n\nDownload the dataset:\n","metadata":{"id":"OaIE0BoZyzgE"}},{"cell_type":"code","source":"#!wget -nv -nc -O $DATASET_PATH $DATASET_URL\n\n\n\nDATASET_URL = \"https://huggingface.co/datasets/vetHealthGuy/Veteran_Affairs_NorthEast_Region_Conversational_Dataset/resolve/main/Updated_VA_Facilities_Conversational_with_Phone.csv\"\n\n!wget -nv -nc -O vet_chat.csv $DATASET_URL","metadata":{"id":"iJSUhANIyqf0","executionInfo":{"status":"ok","timestamp":1731202832412,"user_tz":300,"elapsed":6,"user":{"displayName":"","userId":""}},"outputId":"0900825c-4038-4759-f2fb-2a7d0c574d3b"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Gemma\n\n\n\nIn this step, you will configure Keras precision settings and load Gemma with KerasNLP.\n","metadata":{"id":"BuOc318GVlB9"}},{"cell_type":"code","source":"from keras_nlp.models import GemmaCausalLM\n\n\n\n# Load the Gemma model with built-in preprocessing\n\ngemma_model = GemmaCausalLM.from_preset(\"gemma_2b_en\")\n\nprint(\"Model loaded successfully.\")","metadata":{"id":"zf5AXQumN09U","executionInfo":{"status":"ok","timestamp":1731203220799,"user_tz":300,"elapsed":345048,"user":{"displayName":"","userId":""}},"outputId":"31999a71-3b0d-43cb-ce0f-b472de791bf6"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Keras precision settings\n\n\n\nWhen training on NVIDIA GPUs, mixed precision (`keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")`) can be used to speed up training with minimal effect on training quality. In most cases, it is recommended to turn on mixed precision as it saves both memory and time. However, be aware that at small batch sizes, it can inflate memory usage by 1.5x (weights will be loaded twice, at half precision and full precision).\n\n\n\nFor inference, half-precision (`keras.config.set_floatx(\"bfloat16\")`) will work and save memory (while mixed-precision is not applicable).\n\n\n\nConfigure your precision settings:\n","metadata":{"id":"k54KOGzSNrGz"}},{"cell_type":"code","source":"# Run inferences at half precision\n\nkeras.config.set_floatx(\"bfloat16\")\n\n\n\n# Train at mixed precision (enable for large batch sizes)\n\n#keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")","metadata":{"id":"JodwwVgROfuZ","executionInfo":{"status":"ok","timestamp":1731203220800,"user_tz":300,"elapsed":5,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model summary\n\n\n\nLoad the Gemma model using the `GemmaCausalLM.from_preset()` method:\n","metadata":{"id":"Lj7bzTamp7_f"}},{"cell_type":"code","source":"#print(keras_nlp.models.GemmaCausalLM.presets.keys())","metadata":{"id":"-XpHEhhyarQk"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_NAME = \"gemma_2b_en\"\n\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(MODEL_NAME)","metadata":{"id":"m3oX1KzAN_to","executionInfo":{"status":"ok","timestamp":1731203242096,"user_tz":300,"elapsed":21300,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Display the model summary:\n","metadata":{"id":"GbXgDqKZ5I8_"}},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"id":"kc77gyOR4xNh","outputId":"f91ef522-9529-44f8-9ae5-e7e56aae8bb4","executionInfo":{"status":"ok","timestamp":1731025985109,"user_tz":300,"elapsed":9,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Test examples\n\n\n\nDefine test examples and functions that will be used to test models before and after finetuning:\n","metadata":{"id":"LicAd7yjyRiQ"}},{"cell_type":"code","source":"TEST_EXAMPLES = [\n\n    \"I need to go to the VA in Philadelphia\",\n\n    \"How far is the Tulsa Veteran Affairs from Bixby?\",\n\n    \"Need to travel to New York.  Is there a VA?\",\n\n    \"What is the number to VA in boston\"\n\n]\n\n\n\n# Prompt template for the training data and the finetuning tests\n\nPROMPT_TEMPLATE = \"Instruction:\\n{Prompt}\\n\\nResponse:\\n{Response}\"\n\n\n\nTEST_PROMPTS = [\n\n    PROMPT_TEMPLATE.format(Prompt=example, Response=\"\")\n\n    for example in TEST_EXAMPLES\n\n]","metadata":{"id":"LybWJAJeylhB","executionInfo":{"status":"ok","timestamp":1731203242096,"user_tz":300,"elapsed":6,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Samplers\n\n\n\nYou can control how tokens are generated for `GemmaCausalLM` by calling the `compile()` method with the `sampler` parameter.\n\n\n\nFor example:\n\n\n\n- `greedy`: picks the next token with the largest probability\n\n- `top_k`: randomly picks the next token from the tokens of top K probability\n\n\n\nTo get deterministic outputs in this notebook, make sure you're using the `greedy` sampler:\n","metadata":{"id":"hm8bHp160oh6"}},{"cell_type":"code","source":"gemma_lm.compile(sampler=\"greedy\")","metadata":{"id":"0Gi17ZmD0-WY","executionInfo":{"status":"ok","timestamp":1731203242096,"user_tz":300,"elapsed":6,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To learn more about available samplers, see [Samplers](https://keras.io/api/keras_nlp/samplers).\n","metadata":{"id":"9m5EAkkAsbK5"}},{"cell_type":"markdown","source":"### Inference before finetuning\n\n\n\nCheck how the model responds to the test examples:\n","metadata":{"id":"Akqc9HoM8e9X"}},{"cell_type":"code","source":"for test_example in TEST_EXAMPLES:\n\n    response = gemma_lm.generate(test_example, max_length=48)\n\n    output = response[len(test_example) :]\n\n    print(f\"{test_example}\\n{output!r}\\n\")","metadata":{"id":"Ue-LRnJ_9Mv1","outputId":"fbb14c4a-123d-43fe-9d14-2c7523b048f7","executionInfo":{"status":"ok","timestamp":1730994855464,"user_tz":300,"elapsed":50862,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A pretrained model can generate text that deviates from the output you are expecting. Here are some examples:\n\n\n\n- The output doesn't follow your output requirements.\n\n- The output is too generic or not consistent enough.\n\n- The output is factually incorrect or outdated.\n\n- The output must be aligned with your specific safety policies.\n\n\n\nMore specific inputs (prompt engineering) can fix some of these issues, at the expense of more complex and longer prompts. If the expected output is not part of the model training data, LLMs generate plausible text anyway and produce what is sometimes called hallucinations.\n\n\n\nYou can perform a model finetuning to improve the performance of the model and keep simpler prompts.\n","metadata":{"id":"1zkSfGlQ6ftN"}},{"cell_type":"markdown","source":"## Finetune Gemma\n\n\n\nFinetune your Gemma model to improve its performance in the specific task of answering questions more consistently and more factually.\n","metadata":{"id":"oVJ9b5tWOlyn"}},{"cell_type":"markdown","source":"### Training data\n\n\n\nGenerate the training examples using the dataset:\n","metadata":{"id":"Tk7LUHcCqj1N"}},{"cell_type":"code","source":"NEW_DATASET_PATH = \"vet_chat.csv\"\n\n\n\ndef generate_training_data(training_ratio: int = 100) -> list[str]:\n\n    assert 0 < training_ratio <= 100\n\n    data = []\n\n    # Open the CSV file instead of using readlines() for JSON lines\n\n    with open(NEW_DATASET_PATH, newline='') as file:\n\n        reader = csv.DictReader(file)  # Use DictReader to read CSV into a dictionary\n\n        for row in reader:\n\n            # Skip examples with context, for simplicity (if 'context' exists in your data)\n\n            if row.get(\"context\"):\n\n                continue\n\n            # Format using questionText and answerText from the CSV\n\n            data.append(PROMPT_TEMPLATE.format(Prompt=row['Prompt'], Response=row['Response']))\n\n    total_data_count = len(data)\n\n    training_data_count = total_data_count * training_ratio // 100\n\n    print(f\"Training examples: {training_data_count}/{total_data_count}\")\n\n\n\n    return data[:training_data_count]\n\n\n\n# Limit to 10% for test purposes\n\ntraining_data = generate_training_data(training_ratio=10)","metadata":{"id":"q9YLaaPpvODd","outputId":"acf03711-958c-4f73-90aa-04e1accd5ca4","executionInfo":{"status":"ok","timestamp":1731203242096,"user_tz":300,"elapsed":5,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Low-Rank Adaptation (LoRA)\n\n\n\n[Low Rank Adaptation](https://arxiv.org/abs/2106.09685) (LoRA) is a finetuning technique which greatly reduces the number of trainable parameters for downstream tasks by freezing the full weights of the model and inserting a smaller number of new trainable weights into the model. This technique makes training much faster and more memory-efficient.\n\n\n\nEnable LoRA for the model and set the LoRA rank to 4:\n","metadata":{"id":"qm89dWZU52q0"}},{"cell_type":"code","source":"gemma_lm.backbone.enable_lora(rank=4)","metadata":{"id":"iN-I5RhJC1fh","executionInfo":{"status":"ok","timestamp":1731203242096,"user_tz":300,"elapsed":4,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check that the number of trainable parameters is significantly reduced:\n","metadata":{"id":"DGVreHDoDOkO"}},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"id":"YCcKAxodDN0S","outputId":"bb175bed-7da8-486b-8e13-df6a57a75b8b","executionInfo":{"status":"ok","timestamp":1730994969541,"user_tz":300,"elapsed":508,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The number of trainable parameters decreased from 2.5B down to 1.4M (1,800x less), making it possible to finetune the model with reasonable GPU memory requirements.\n","metadata":{"id":"x8dyzB5MXfo0"}},{"cell_type":"markdown","source":"### Finetuning\n\n\n\nFinetune the model with the training data. This step can take a couple of minutes:\n","metadata":{"id":"bsw_-qIGCZnv"}},{"cell_type":"code","source":"\n\ndef finetune_gemma(model: keras_nlp.models.GemmaCausalLM, data: list[str]):\n\n    # Reduce the input sequence length to limit memory usage\n\n    model.preprocessor.sequence_length = 64\n\n\n\n    # Define early stopping\n\n    early_stopping = EarlyStopping(monitor='loss', patience=1)\n\n\n\n    # Configure the AdamW optimizer\n\n    optimizer = keras.optimizers.AdamW(\n\n        learning_rate=5e-6,\n\n        weight_decay=0.01,\n\n    )\n\n    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])  # Exclude bias and layer norm from decay\n\n\n\n    # Convert data to a tf.data.Dataset and batch it\n\n    dataset = tf.data.Dataset.from_tensor_slices(data).batch(4)\n\n\n\n    # Compile the model\n\n    model.compile(\n\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n\n        optimizer=optimizer,\n\n        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n\n    )\n\n\n\n    # Fit model with batched dataset\n\n    model.fit(dataset, epochs=4, callbacks=[early_stopping])\n\n\n\n    # Save the model in .keras format\n\n    model.save('veteran_assistance_finetuned_model.keras')\n\n    print(\"Model saved as veteran_assistance_finetuned_model.keras\")\n\n\n\n    # Save the model in .h5 format\n\n    model.save('veteran_assistance_finetuned_model.h5', save_format='h5')\n\n    print(\"Model saved as veteran_assistance_finetuned_model.h5\")\n\n\n\n# Sample function call (assuming you have `gemma_lm` and `training_data`)\n\nfinetune_gemma(gemma_lm, training_data)\n","metadata":{"id":"mfk9X11tvPWy","outputId":"44122f7b-7e1f-447c-83bb-d38995ef0b3f","executionInfo":{"status":"error","timestamp":1731204511186,"user_tz":300,"elapsed":4,"user":{"displayName":"","userId":""}}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Inference after finetuning\n\n\n\nTest the finetuned model:\n","metadata":{"id":"twNI1SKd-4Sp"}},{"cell_type":"markdown","source":"### Reloading the models after initial training","metadata":{"id":"oo5kNvKJq0Tz"}},{"cell_type":"code","source":"import keras_hub\n\n\n\n# Check available attributes and methods of GemmaBackbone\n\nprint(dir(keras_hub.models.GemmaBackbone))\n","metadata":{"id":"gDYaAdS3z3wb","executionInfo":{"status":"ok","timestamp":1731204191866,"user_tz":300,"elapsed":3,"user":{"displayName":"","userId":""}},"outputId":"b9187e51-43b6-4488-aa9a-6f6679f8e2f5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras_hub\n\n\n\n# Step 1: Initialize GemmaBackbone with the \"gemma_2b_en\" preset\n\nbackbone = keras_hub.models.GemmaBackbone.from_preset(\"gemma_2b_en\")\n\n\n\n# Step 2: Initialize GemmaCausalLM with this backbone, skipping the preprocessor\n\ngemma_lm = keras_hub.models.GemmaCausalLM(\n\n    backbone=backbone,\n\n    preprocessor=None  # Set to None to skip tokenization\n\n)\n\n\n\n# Step 3: Load the previously saved weights, if any\n\ngemma_lm.load_weights(\"veteran_assistance_finetuned_model.h5\")\n\n\n\n# Define test prompts with raw text input\n\nTEST_PROMPTS = [\n\n    \"What services are available for veterans at the VA in Philadelphia?\",\n\n    \"How can I get healthcare benefits as a veteran?\",\n\n    \"Are there mental health resources at the VA?\",\n\n]\n\n\n\n# Step 4: Generate responses without any tokenization\n\nfor prompt in TEST_PROMPTS:\n\n    # Directly pass the raw prompt to the generate method\n\n    output = gemma_lm.generate(prompt, max_length=40)\n\n    print(f\"Prompt: {prompt}\")\n\n    print(f\"Response: {output}\\n{'- ' * 40}\")\n","metadata":{"id":"fmxFo7fXqz6U","executionInfo":{"status":"error","timestamp":1731204404409,"user_tz":300,"elapsed":21738,"user":{"displayName":"","userId":""}},"outputId":"d062a9ba-e875-468d-c876-a49d545f6851"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You should observe that outputs are now structured, more consistent, and more factual.\n","metadata":{"id":"XyraRakQrz3i"}},{"cell_type":"markdown","source":"## Retraining of the model\n","metadata":{"id":"Utq0ChwVtk5E"}},{"cell_type":"code","source":"import torch\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments\n\nfrom torch.utils.data import Dataset, DataLoader\n\n\n\n# Define the file path for data and model\n\ndata_path = '/content/vet_chat.csv'\n\nmodel_path = '/content/veteran_assistance_finetuned_model.h5'\n\n\n\n# Load data in a memory-efficient way\n\nclass VAInstructionsDataset(Dataset):\n\n    def __init__(self, data_path, tokenizer, max_length=128):\n\n        self.data = pd.read_json(data_path, lines=True)\n\n        self.tokenizer = tokenizer\n\n        self.max_length = max_length\n\n\n\n    def __len__(self):\n\n        return len(self.data)\n\n\n\n    def __getitem__(self, idx):\n\n        item = self.data.iloc[idx]\n\n        instruction = item['Instruction']\n\n        response = item['Response']\n\n\n\n        inputs = self.tokenizer(\n\n            instruction,\n\n            max_length=self.max_length,\n\n            padding='max_length',\n\n            truncation=True,\n\n            return_tensors='pt'\n\n        )\n\n        outputs = self.tokenizer(\n\n            response,\n\n            max_length=self.max_length,\n\n            padding='max_length',\n\n            truncation=True,\n\n            return_tensors='pt'\n\n        )\n\n\n\n        input_ids = inputs['input_ids'].squeeze()\n\n        attention_mask = inputs['attention_mask'].squeeze()\n\n        labels = outputs['input_ids'].squeeze()\n\n\n\n        return {\n\n            'input_ids': input_ids,\n\n            'attention_mask': attention_mask,\n\n            'labels': labels,\n\n        }\n\n\n\n# Load tokenizer and model\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n\n\n\n# Prepare dataset and dataloader\n\ndataset = VAInstructionsDataset(data_path, tokenizer)\n\ntrain_dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n\n\n\n# Training arguments\n\ntraining_args = TrainingArguments(\n\n    output_dir='/content/model-finetuned',\n\n    num_train_epochs=3,\n\n    per_device_train_batch_size=4,\n\n    save_steps=500,\n\n    save_total_limit=2,\n\n    logging_dir='/content/logs',\n\n    logging_steps=100,\n\n    load_best_model_at_end=True,\n\n)\n\n\n\n# Trainer\n\ntrainer = Trainer(\n\n    model=model,\n\n    args=training_args,\n\n    train_dataset=dataset,\n\n    tokenizer=tokenizer,\n\n)\n\n\n\n# Fine-tune the model\n\ntrainer.train()\n\n\n\n# Save the model\n\nmodel.save_pretrained('/content/model-finetuned')\n\ntokenizer.save_pretrained('/content/model-finetuned')","metadata":{"id":"PJHXk-stX8pk","executionInfo":{"status":"error","timestamp":1731012450508,"user_tz":300,"elapsed":6732,"user":{"displayName":"","userId":""}},"outputId":"aca24a43-3c67-4995-8b6b-78e1518dea92"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"XkITIUOkYa8u"},"outputs":[],"execution_count":null}]}